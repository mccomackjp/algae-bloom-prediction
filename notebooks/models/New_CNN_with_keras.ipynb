{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, confusion_matrix, precision_score\n",
    "from scripts.model_functions import create_model_mult, create_model\n",
    "import matplotlib.pylab as plt\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables to be used in Keras and the CNN\n",
    "\n",
    "# number of items to use for training\n",
    "BATCH_SIZE = 100 \n",
    "\n",
    "# Number of identifying classes \n",
    "#   WE have two, Bloom and no bloom 1/0\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "# number of times to repeat process\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "df_train = pd.read_csv('../../data/cleaned/site1_vineyard.csv')\n",
    "df_test = pd.read_csv('../../data/cleaned/site2_bird.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date (mm.dd.yyyy)</th>\n",
       "      <th>Time 24hr</th>\n",
       "      <th>Temp C</th>\n",
       "      <th>Sp Cond (uS/cm)</th>\n",
       "      <th>pH (mV)</th>\n",
       "      <th>pH</th>\n",
       "      <th>Turbidity (NTU)</th>\n",
       "      <th>ODOSat%</th>\n",
       "      <th>ODO (mg/L)</th>\n",
       "      <th>BGA-Phycocyanin RFU</th>\n",
       "      <th>BGA (ug/L)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/5/2017</td>\n",
       "      <td>0:00</td>\n",
       "      <td>15.37</td>\n",
       "      <td>2184</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.70</td>\n",
       "      <td>92.2</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.428449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/5/2017</td>\n",
       "      <td>0:15</td>\n",
       "      <td>15.45</td>\n",
       "      <td>2139</td>\n",
       "      <td>-101.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>9.92</td>\n",
       "      <td>93.3</td>\n",
       "      <td>9.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.428449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/5/2017</td>\n",
       "      <td>0:30</td>\n",
       "      <td>15.49</td>\n",
       "      <td>2057</td>\n",
       "      <td>-102.3</td>\n",
       "      <td>8.45</td>\n",
       "      <td>8.90</td>\n",
       "      <td>94.8</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.856898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/5/2017</td>\n",
       "      <td>0:45</td>\n",
       "      <td>15.67</td>\n",
       "      <td>1978</td>\n",
       "      <td>-102.6</td>\n",
       "      <td>8.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.49</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.856898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/5/2017</td>\n",
       "      <td>1:00</td>\n",
       "      <td>15.34</td>\n",
       "      <td>2136</td>\n",
       "      <td>-100.2</td>\n",
       "      <td>8.41</td>\n",
       "      <td>9.88</td>\n",
       "      <td>92.7</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.428449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date (mm.dd.yyyy) Time 24hr  Temp C  Sp Cond (uS/cm)  pH (mV)    pH  \\\n",
       "0          5/5/2017      0:00   15.37             2184   -100.0  8.41   \n",
       "1          5/5/2017      0:15   15.45             2139   -101.0  8.43   \n",
       "2          5/5/2017      0:30   15.49             2057   -102.3  8.45   \n",
       "3          5/5/2017      0:45   15.67             1978   -102.6  8.45   \n",
       "4          5/5/2017      1:00   15.34             2136   -100.2  8.41   \n",
       "\n",
       "   Turbidity (NTU)  ODOSat%  ODO (mg/L)  BGA-Phycocyanin RFU  BGA (ug/L)  \n",
       "0            10.70     92.2        9.16                  0.1    0.428449  \n",
       "1             9.92     93.3        9.25                  0.1    0.428449  \n",
       "2             8.90     94.8        9.40                  0.2    0.856898  \n",
       "3             8.62     96.0        9.49                  0.2    0.856898  \n",
       "4             9.88     92.7        9.22                  0.1    0.428449  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df_train['BGA-Phycocyanin RFU'].apply(lambda x : x/0.2334)\n",
    "df_train = df_train.drop(columns=['Chlorophyll (ug/L)', 'Chlorophyll RFU'])\n",
    "df_train['BGA (ug/L)'] = target\n",
    "df_train.head(5)\n",
    "\n",
    "\n",
    "target = df_test['BGA-Phycocyanin RFU'].apply(lambda x : x/0.2334)\n",
    "df_test = df_test.drop(columns=['Chlorophyll (ug/L)', 'Chlorophyll RFU'])\n",
    "df_test['BGA (ug/L)'] = target\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = df_train['Date (mm.dd.yyyy)'] + ' '+ df_train['Time 24hr']\n",
    "timestamp = pd.to_datetime(timestamp)\n",
    "df_train['Timestamp'] = timestamp\n",
    "\n",
    "timestamp = df_test['Date (mm.dd.yyyy)'] + ' '+ df_test['Time 24hr']\n",
    "timestamp = pd.to_datetime(timestamp)\n",
    "df_test['Timestamp'] = timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont need data and time now that we have Timestamp. Lets remove them\n",
    "df_train = df_train.drop(columns=['Date (mm.dd.yyyy)', 'Time 24hr'])\n",
    "df_test = df_test.drop(columns=['Date (mm.dd.yyyy)', 'Time 24hr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = df_train['BGA (ug/L)'].apply(lambda x: 1 if x > 20 else 0)\n",
    "df_train['Bloom'] = train_target\n",
    "\n",
    "test_target = df_test['BGA (ug/L)'].apply(lambda x: 1 if x > 20 else 0)\n",
    "df_test['Bloom'] = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# lets try to normalize this now....\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataset_columns = ['Temp C','Sp Cond (uS/cm)', 'pH (mV)','pH', 'Turbidity (NTU)', 'ODOSat%','ODO (mg/L)', 'Bloom']\n",
    "scaler = MinMaxScaler()\n",
    "ds_scaled = scaler.fit_transform(df_train[dataset_columns])\n",
    "df_train = pd.DataFrame(ds_scaled,columns=dataset_columns)\n",
    "\n",
    "ds_scaled = scaler.fit_transform(df_test[dataset_columns])\n",
    "df_test = pd.DataFrame(ds_scaled,columns=dataset_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to take a moving window of the data of 10 time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "determines the window size for the daata set\n",
    "@param dataset - The dataset to get windows for\n",
    "@param window_size - the size of the window  \n",
    "@param shift - the amout to shift the window\n",
    "'''\n",
    "def windows(dataset, window_size, shift):\n",
    "    start = 0\n",
    "    while start+window_size < dataset.shape[0]: \n",
    "        yield (int(start), int(start+window_size))\n",
    "        # shift the window five blocks of time\n",
    "        start += shift\n",
    "        if start % 300 == 0:\n",
    "            print('Window Segmentation {0:.2f}% done'.format(((start+window_size) / dataset.shape[0]) * 100 ))\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Segments the dataset based on the parameters that are passed in.\n",
    "@param dataset - the dataset to segment into window\n",
    "@param columns - the array of columns from the dataset to be looked at\n",
    "@param window_size - the size of the window you would like to be looked at. Defualt is 10\n",
    "\n",
    "'''\n",
    "def segment_dataset(dataset, columns, target, window_size=10):    \n",
    "    print('WINDOW SIZE',window_size)\n",
    "    print('NUMBER OF COULUMNS',len(columns))\n",
    "    segments = np.empty((0, window_size, len(columns)))\n",
    "    labels = np.empty((0))\n",
    "    count = 0\n",
    "    for (start, end) in windows(dataset, window_size, 1):\n",
    "        count+=1\n",
    "        values = dataset[columns][start:end]\n",
    "        if(values.shape[0] == window_size):\n",
    "            segments = np.vstack([segments, np.stack([values])])\n",
    "            # Takes the larger of the two variables if there are more than one. \n",
    "            # This makes it more likly to predict a bloom. Can be changed to iloc[0] to\n",
    "            # be less likly to predict a bloom (more 0s in the label array)\n",
    "            \n",
    "            labels = np.append(labels, dataset[target][start:end].mode().iloc[-1])\n",
    "        else:\n",
    "            print(\"No more Windows available... Exiting\")\n",
    "            break\n",
    "    return (segments, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINDOW SIZE 9\n",
      "NUMBER OF COULUMNS 7\n",
      "Window Segmentation 1.63% done\n",
      "Window Segmentation 3.21% done\n",
      "Window Segmentation 4.80% done\n",
      "Window Segmentation 6.38% done\n",
      "Window Segmentation 7.96% done\n",
      "Window Segmentation 9.55% done\n",
      "Window Segmentation 11.13% done\n",
      "Window Segmentation 12.71% done\n",
      "Window Segmentation 14.30% done\n",
      "Window Segmentation 15.88% done\n",
      "Window Segmentation 17.46% done\n",
      "Window Segmentation 19.05% done\n",
      "Window Segmentation 20.63% done\n",
      "Window Segmentation 22.21% done\n",
      "Window Segmentation 23.80% done\n",
      "Window Segmentation 25.38% done\n",
      "Window Segmentation 26.96% done\n",
      "Window Segmentation 28.55% done\n",
      "Window Segmentation 30.13% done\n",
      "Window Segmentation 31.71% done\n",
      "Window Segmentation 33.30% done\n",
      "Window Segmentation 34.88% done\n",
      "Window Segmentation 36.46% done\n",
      "Window Segmentation 38.05% done\n",
      "Window Segmentation 39.63% done\n",
      "Window Segmentation 41.21% done\n",
      "Window Segmentation 42.80% done\n",
      "Window Segmentation 44.38% done\n",
      "Window Segmentation 45.97% done\n",
      "Window Segmentation 47.55% done\n",
      "Window Segmentation 49.13% done\n",
      "Window Segmentation 50.72% done\n",
      "Window Segmentation 52.30% done\n",
      "Window Segmentation 53.88% done\n",
      "Window Segmentation 55.47% done\n",
      "Window Segmentation 57.05% done\n",
      "Window Segmentation 58.63% done\n",
      "Window Segmentation 60.22% done\n",
      "Window Segmentation 61.80% done\n",
      "Window Segmentation 63.38% done\n",
      "Window Segmentation 64.97% done\n",
      "Window Segmentation 66.55% done\n",
      "Window Segmentation 68.13% done\n",
      "Window Segmentation 69.72% done\n",
      "Window Segmentation 71.30% done\n",
      "Window Segmentation 72.88% done\n",
      "Window Segmentation 74.47% done\n",
      "Window Segmentation 76.05% done\n",
      "Window Segmentation 77.63% done\n",
      "Window Segmentation 79.22% done\n",
      "Window Segmentation 80.80% done\n",
      "Window Segmentation 82.38% done\n",
      "Window Segmentation 83.97% done\n",
      "Window Segmentation 85.55% done\n",
      "Window Segmentation 87.13% done\n",
      "Window Segmentation 88.72% done\n",
      "Window Segmentation 90.30% done\n",
      "Window Segmentation 91.88% done\n",
      "Window Segmentation 93.47% done\n",
      "Window Segmentation 95.05% done\n",
      "Window Segmentation 96.63% done\n",
      "Window Segmentation 98.22% done\n",
      "Window Segmentation 99.80% done\n",
      "WINDOW SIZE 9\n",
      "NUMBER OF COULUMNS 7\n",
      "Window Segmentation 1.81% done\n",
      "Window Segmentation 3.56% done\n",
      "Window Segmentation 5.32% done\n",
      "Window Segmentation 7.07% done\n",
      "Window Segmentation 8.83% done\n",
      "Window Segmentation 10.58% done\n",
      "Window Segmentation 12.34% done\n",
      "Window Segmentation 14.09% done\n",
      "Window Segmentation 15.85% done\n",
      "Window Segmentation 17.60% done\n",
      "Window Segmentation 19.36% done\n",
      "Window Segmentation 21.11% done\n",
      "Window Segmentation 22.87% done\n",
      "Window Segmentation 24.62% done\n",
      "Window Segmentation 26.38% done\n",
      "Window Segmentation 28.13% done\n",
      "Window Segmentation 29.89% done\n",
      "Window Segmentation 31.64% done\n",
      "Window Segmentation 33.40% done\n",
      "Window Segmentation 35.15% done\n",
      "Window Segmentation 36.91% done\n",
      "Window Segmentation 38.66% done\n",
      "Window Segmentation 40.42% done\n",
      "Window Segmentation 42.17% done\n",
      "Window Segmentation 43.93% done\n",
      "Window Segmentation 45.68% done\n",
      "Window Segmentation 47.43% done\n",
      "Window Segmentation 49.19% done\n",
      "Window Segmentation 50.94% done\n",
      "Window Segmentation 52.70% done\n",
      "Window Segmentation 54.45% done\n",
      "Window Segmentation 56.21% done\n",
      "Window Segmentation 57.96% done\n",
      "Window Segmentation 59.72% done\n",
      "Window Segmentation 61.47% done\n",
      "Window Segmentation 63.23% done\n",
      "Window Segmentation 64.98% done\n",
      "Window Segmentation 66.74% done\n",
      "Window Segmentation 68.49% done\n",
      "Window Segmentation 70.25% done\n",
      "Window Segmentation 72.00% done\n",
      "Window Segmentation 73.76% done\n",
      "Window Segmentation 75.51% done\n",
      "Window Segmentation 77.27% done\n",
      "Window Segmentation 79.02% done\n",
      "Window Segmentation 80.78% done\n",
      "Window Segmentation 82.53% done\n",
      "Window Segmentation 84.29% done\n",
      "Window Segmentation 86.04% done\n",
      "Window Segmentation 87.80% done\n",
      "Window Segmentation 89.55% done\n",
      "Window Segmentation 91.31% done\n",
      "Window Segmentation 93.06% done\n",
      "Window Segmentation 94.82% done\n",
      "Window Segmentation 96.57% done\n",
      "Window Segmentation 98.33% done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "feature_columns = dataset_columns[:-1]\n",
    "(x_train, y_train) = segment_dataset(df_train, feature_columns, 'Bloom', 9)\n",
    "(x_test, y_test) = segment_dataset(df_test, feature_columns, 'Bloom', 9)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18938, 9, 7)\n",
      "(17086, 9, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18938,)\n",
      "(17086,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaping the data to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(len(x_train),9,7,1)\n",
    "x_test = x_test.reshape(len(x_test),9,7,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking apart training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (18938, 9, 7, 1)\n",
      "x_test shape: (17086, 9, 7, 1)\n",
      "y_train shape: (18938, 1)\n",
      "y_test shape: (17086, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\",x_train.shape)\n",
    "print(\"x_test shape:\",x_test.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "print(\"y_test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mod = ks.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_mod = ks.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "input_shape = (9,7,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the recall of the different metrics\n",
    "def create_class_predictions(pred):\n",
    "    retval = np.array([])\n",
    "    for row in pred:\n",
    "        max_value = (-1,-1)\n",
    "        for index, value in enumerate(row):\n",
    "            if value > max_value[1]:\n",
    "                max_value = (index, value)\n",
    "        retval = np.append(retval, max_value[0])\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Come on, let's create the model already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...And Here we go....\n",
      "Started at 2019-02-09 11:28:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.00% complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e6afceaf1320>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m101\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# What is our score?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "values = []\n",
    "\n",
    "\n",
    "print(\"...And Here we go....\")\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Started at\",st)\n",
    "for i in range(2, 101, 2):\n",
    "    model = create_model(i, 7, input_shape, NUM_CLASSES,0.0001)\n",
    "    model.fit(x=x_train, y=y_train_mod, batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=0)\n",
    "    # What is our score?\n",
    "    score = model.evaluate(x_train, y_train_mod, verbose=0)\n",
    "    predictions = model.predict(x_test)\n",
    "    predict = create_class_predictions(predictions)\n",
    "    recall = recall_score(y_test.reshape(-1,), predict)\n",
    "    precision = precision_score(y_test.reshape(-1,), predict)\n",
    "    value = (i, recall, precision)\n",
    "    values.append(value)\n",
    "    print('{0:.2f}% complete'.format(i))\n",
    "\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Finished at\",st)\n",
    "\n",
    "#This created the values that are listed in the following cell. but took 3 hours to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "values\n",
    "\n",
    "[(2, 0.0), (4, 0.0), (6, 0.0), (8, 0.0), (10, 0.0), (12, 0.3333333333333333), (14, 0.16666666666666666), (16, 0.3333333333333333), (18, 0.3333333333333333), (20, 0.6666666666666666), (22, 0.5), (24, 0.5), (26, 0.6666666666666666), (28, 0.6666666666666666), (30, 0.3333333333333333), (32, 0.3333333333333333), (34, 0.0), (36, 0.3333333333333333), (38, 0.3333333333333333), (40, 0.3333333333333333), (42, 0.3333333333333333), (44, 1.0), (46, 0.3333333333333333), (48, 0.3333333333333333), (50, 0.3333333333333333), (52, 0.3333333333333333), (54, 0.3333333333333333), (56, 0.3333333333333333), (58, 0.3333333333333333), (60, 0.6666666666666666), (62, 0.6666666666666666), (64, 0.3333333333333333), (66, 0.3333333333333333), (68, 0.6666666666666666), (70, 0.3333333333333333), (72, 0.3333333333333333), (74, 0.5), (76, 0.3333333333333333), (78, 1.0), (80, 0.3333333333333333), (82, 0.3333333333333333), (84, 0.3333333333333333), (86, 0.3333333333333333), (88, 0.3333333333333333), (90, 0.6666666666666666), (92, 0.5), (94, 0.5), (96, 0.3333333333333333), (98, 0.3333333333333333), (100, 0.3333333333333333)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take the top performing ones and see if the n +/- 1 of the output layers will get better /similar performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...And Here we go....\n",
      "Started at 2019-01-11 11:28:42\n",
      "3.85% complete\n",
      "7.69% complete\n",
      "11.54% complete\n",
      "15.38% complete\n",
      "19.23% complete\n",
      "23.08% complete\n",
      "26.92% complete\n",
      "30.77% complete\n",
      "34.62% complete\n",
      "38.46% complete\n",
      "42.31% complete\n",
      "46.15% complete\n",
      "50.00% complete\n",
      "53.85% complete\n",
      "57.69% complete\n",
      "61.54% complete\n",
      "65.38% complete\n",
      "69.23% complete\n",
      "73.08% complete\n",
      "76.92% complete\n",
      "80.77% complete\n",
      "84.62% complete\n",
      "88.46% complete\n",
      "92.31% complete\n",
      "96.15% complete\n",
      "100.00% complete\n",
      "Finished at 2019-01-11 12:05:54\n"
     ]
    }
   ],
   "source": [
    "number_array = [19,20,21,25,26,27,27,28,29,43,44,45,59,60,61,62,63,67,68,69,77,78,79,89,90,91]\n",
    "values = []\n",
    "\n",
    "print(\"...And Here we go....\")\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Started at\",st)\n",
    "count = 0\n",
    "for i in number_array:\n",
    "    count +=1\n",
    "    model = create_model(i, 7, input_shape, NUM_CLASSES,0.0001)\n",
    "    model.fit(x=x_train, y=y_train_mod, batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=0)\n",
    "    # What is our score?\n",
    "    score = model.evaluate(x_train, y_train_mod, verbose=0)\n",
    "    predictions = model.predict(x_test)\n",
    "    predict = create_class_predictions(predictions)\n",
    "    recall = recall_score(y_test.reshape(-1,), predict)\n",
    "    value = (i, recall)\n",
    "    values.append(value)\n",
    "    print('{0:.2f}% complete'.format((count / len(number_array)) * 100))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, 7, input_shape=input_shape, activation='relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(15))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(loss=ks.losses.categorical_crossentropy,\n",
    "              optimizer=ks.optimizers.Adam(lr=0.0001),\n",
    "              metrics=[precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19, 0.3), (20, 0.26), (21, 0.5), (25, 0.56), (26, 0.72), (27, 0.3), (27, 0.58), (28, 0.36), (29, 0.5), (43, 0.76), (44, 0.6), (45, 0.9), (59, 0.7), (60, 0.68), (61, 0.64), (62, 0.74), (63, 0.68), (67, 0.56), (68, 0.68), (69, 0.3), (77, 0.76), (78, 0.48), (79, 0.66), (89, 0.96), (90, 0.68), (91, 0.7)]\n"
     ]
    }
   ],
   "source": [
    "print(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...And Here we go....\n",
      "Started at 2018-12-22 13:25:43\n",
      "14\n",
      "0.04% complete\n",
      "22\n",
      "0.08% complete\n",
      "29\n",
      "0.12% complete\n",
      "33\n",
      "0.17% complete\n",
      "44\n",
      "0.21% complete\n",
      "58\n",
      "0.25% complete\n",
      "66\n",
      "0.29% complete\n",
      "73\n",
      "0.33% complete\n",
      "77\n",
      "0.38% complete\n",
      "88\n",
      "0.42% complete\n",
      "102\n",
      "0.46% complete\n",
      "110\n",
      "0.50% complete\n",
      "117\n",
      "0.54% complete\n",
      "121\n",
      "0.58% complete\n",
      "132\n",
      "0.62% complete\n",
      "146\n",
      "0.67% complete\n",
      "154\n",
      "0.71% complete\n",
      "161\n",
      "0.75% complete\n",
      "165\n",
      "0.79% complete\n",
      "176\n",
      "0.83% complete\n",
      "190\n",
      "0.88% complete\n",
      "198\n",
      "0.92% complete\n",
      "205\n",
      "0.96% complete\n",
      "209\n",
      "1.00% complete\n",
      "Finished at 2018-12-22 14:41:34\n"
     ]
    }
   ],
   "source": [
    "# This is to see if the increasing or decreasing of the output array for the second conv layer will help with predicion.\n",
    "\n",
    "mult_array = [0.33,0.5,0.66,0.75,1,1.33,1.5,1.66,1.75,2,2.33,2.5,2.66,2.75,3,3.33,3.5,3.66,3.75,4,4.33,4.5,4.66,4.75]\n",
    "values = []\n",
    "\n",
    "print(\"...And Here we go....\")\n",
    "st = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Started at\",st)\n",
    "count = 0\n",
    "for i in mult_array:\n",
    "    count +=1\n",
    "    model = create_model_mult(i, 7, input_shape, NUM_CLASSES,0.0001)\n",
    "    model.fit(x=x_train, y=y_train_mod, batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=0)\n",
    "    # What is our score?\n",
    "    score = model.evaluate(x_train, y_train_mod, verbose=0)\n",
    "    predictions = model.predict(x_test)\n",
    "    predict = create_class_predictions(predictions)\n",
    "    recall = recall_score(y_test.reshape(-1,), predict)\n",
    "    value = (i, recall)\n",
    "    values.append(value)\n",
    "    print('{0:.2f}% complete'.format(count / len(mult_array)))\n",
    "\n",
    "st = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Finished at\",st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.33, 0.92), (0.5, 0.72), (0.66, 1.0), (0.75, 0.98), (1, 0.94), (1.33, 0.82), (1.5, 0.92), (1.66, 1.0), (1.75, 0.84), (2, 0.98), (2.33, 0.94), (2.5, 0.82), (2.66, 0.8), (2.75, 0.98), (3, 0.6), (3.33, 0.98), (3.5, 0.98), (3.66, 0.16), (3.75, 0.94), (4, 1.0), (4.33, 0.94), (4.5, 0.82), (4.66, 0.98), (4.75, 0.4)]\n"
     ]
    }
   ],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mccomackjp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./../../saved_models/cnn_model/1\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# ignoring dropout for deployment\n",
    "K.set_learning_phase(0)\n",
    " \n",
    "# Set a file path to save the model in.\n",
    "model_name = \"cnn_model\"\n",
    "model_version = \"1\"\n",
    "tf_path = \"./../../saved_models/{}/{}\".format(model_name, model_version)\n",
    " \n",
    "# Get the session from the Keras back-end to save the model in TF format.\n",
    "with K.get_session() as sess:\n",
    "    tf.saved_model.simple_save(sess, tf_path, inputs={'input': model.input}, outputs={t.name: t for t in model.outputs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
